---
title: "STAT 333 Project Two"
Author1: "Xinyi Yu  xyu286@wisc.edu"
Author2:  "Xiu Xie   xxie65@wisc.edu"
Authour3: "Yuhan Xie   xie75@wisc.edu"  
Author4: "Yuwen Zhang  zhang924@wisc.edu"
date: 04/20/2019
output: html_document
---

**Author1:    Xinyi Yu        xyu286@wisc.edu**
**Author2:    Xiu Xie         xxie65@wisc.edu**
**Author3:    Yuhan Xie        xie75@wisc.edu**
**Author4:    Yuwen Zhang   zhang924@wisc.edu**

**We will use R markdown to represent our R code.**

**In this block we will complete initial preperation**
**Packages we used in our project are readr and stringr.**
```{r}
library(readr)   #load readr
library(stringr)   #load stringr

rm(list = ls())   #clear all current objects in workspace
chart = read_csv("train_Madison.csv")   #read train_Madison file into chart
```

**In this block we will perform data cleaning to the original dataset**
```{r}
#This is the function where we will recreate all the predictors, the input values we need are "data", which takes in the value in chart, "test", which we initializes to FALSE, "portion", which we initializes to TRUE, "create", which we initializes to TRUE, and "origin", which takes in the value in chart

#Note: 1. if "test" equals TRUE, we are working on the "test_Madison.csv" file
#      2. if "portion" equals TRUE, we are converting all values to the their ratio of the number            of words in each text
#      3. if "create" equals TRUE, we are recreating all predictors in the original dataset
#      4. "origin" is used when recreating the "test_Madison.csv" file, this variable is aimed to            pair up the predictors in the test file with the train file
data_transform = function(data = chart, test = FALSE, portion = TRUE, create = TRUE, origin = chart) {
  
  #The below four lines are aimed to rename the existing predictors "name", "city", "text", and      "star" to avoid confusion when we move to recreating other predictors
  colnames(data)[colnames(data) == "name"] = "NAME"   #Rename the predictor "name" to "NAME"
  colnames(data)[colnames(data) == "city"] = "CITY"   #Rename the predictor "city" to "CITY"
  colnames(data)[colnames(data) == "text"] = "TEXT"   #Rename the predictor "text" to "TEXT"
  colnames(data)[colnames(data) == "star"] = "STAR"   #Rename the predictor "star" to "STAR"
 
  #The below nested if-statements are aimed to initialize the dataset before recreating predictors
  if(create == TRUE){   #Keep the original dataset from the first column to the 8th column
    data = data[1:which(colnames(data) == "nword")]
    if(test == TRUE){
      for(i in (which(colnames(origin) == "nword")+1):length(origin)){
        data$new_col = rep(0, nrow(data))   #Recreate new column and initialize predictors to zeros
        colnames(data)[ncol(data)] = colnames(origin)[i]   #Recreate the names of all the new                                                   predictors in the test file as those in the train file
      }
    }
  }
  
  #The below four lines are aimed to add four new predictors to the dataset
  data$exclam_mark = rep(0, nrow(data))   #Initialize the column "exclam_mark" to zeros
  data$question_mark = rep(0, nrow(data))   #Initialize the column "question_mark" to zeros
  data$dots_count = rep(0, nrow(data))   #Initialize the column "dots_count" to zeros
  data$capital_letter = rep(0, nrow(data))   ##Initialize the column "capital_letter" to zeros
  
  #This nested for-loop is aimed to delete unnecessary predictors and create new predictors
  for(i in 1:nrow(data)){   
    if(i%%1000 == 0){   #For each one thousand rows of the dataset
      print(i)   #Print the number of that row number to keep track of the process
      if(test == FALSE){   #If not working on the "test_Madison" dataset
        delete_index = c()   #Initiate detete_index line
        for(j in (which(colnames(data) == "nword")+1):length(data)){   #For each column after the                                                                         column "nword"
          if(sum(data[j]) < 0.005*i)   #If the sum of all the numbers of that predictor word in                                           less than 0.5% of the current row number
            delete_index = c(delete_index, j)   #Set the delete_index to current column
        }
        data = data[-delete_index]   #Delete this column
        print(length(data))   #Print the number of predictors of keep track of the process
      }
    }
    
    text = data$TEXT[i]   #Set the "text" variable to the text review of current row
    #---------------------------
    if(create == TRUE && test == FALSE){   #While we are recreating the "train_Madison" dataset
      words = str_extract_all(tolower(text), "[a-z]+")
      for(j in 1:length(words[[1]])){
        if(!(words[[1]][j] %in% colnames(data))){
          data$new_col = rep(0, nrow(data))
          data$new_col[i] = 1
          colnames(data)[ncol(data)] = words[[1]][j]
        }else{
          data[i, which(colnames(data) == words[[1]][j])] = data[i, which(colnames(data) == words[[1]][j])] + 1
        }
      }
    }
    if(create == TRUE && test == TRUE){
      words = str_extract_all(tolower(text), "[a-z]+")
      for(j in 1:length(words[[1]])){
        if(words[[1]][j] %in% colnames(data)){
          data[i, which(colnames(data) == words[[1]][j])] = data[i, which(colnames(data) == words[[1]][j])] + 1
        }
      }
    }
    
    #count punctuations
    data$exclam_mark[i] = str_count(text, pattern = "!")
    data$question_mark[i] = str_count(text, pattern = "\\?")
    data$dots_count[i] = str_count(text, pattern = "\\.")
    data$capital_letter[i] = str_count(text, pattern = "[ABCDEFGHIJKLMNOPQRSTUVWXYZ]")
  }
  
  if(test == FALSE){
    delete_index = c()
    for(j in (which(colnames(data) == "nword")+1):length(data)){
      if(sum(data[j]) < 0.005*length(data))
        delete_index = c(delete_index, j)
    }
    data = data[-delete_index]
    print(length(data))
  }
  
  
  #find the most influencial predictors
  if(test == TRUE){
    trim = ((which(colnames(data) == "nword"))+1):length(data)
  }else{
    trim = ((which(colnames(data) == "nword"))+1):length(data)
  }
  
  if(test == TRUE){
    l<-rep(-100, length(origin)) #cor of each predictor with star
    for (i in ((which(colnames(origin) == "dots_count"))+1):length(origin)){
      l[i] = cor(origin$STAR, origin[,i])
    }
  }else{
    l<-rep(-100, length(data)) #cor of each predictor with star
    for (i in ((which(colnames(data) == "dots_count"))+1):length(data)){
      l[i] = cor(data$STAR, data[,i])
    }
  }
  l = l[l!=-100]

  #calculate the number of extreme words
  most = names(data[which(rank(l) > length(data)- 100) + (which(colnames(data) == "dots_count"))])
  least = names(data[which(rank(l) <101) + (which(colnames(data) == "dots_count"))])
 
  most_sum = apply(data[,most], MARGIN = 1, FUN = sum)
  least_sum = apply(data[,least], MARGIN = 1, FUN = sum)
  #-----------------------
  
  data$PORTION = (most_sum+1) / (least_sum+1)
  
  data$PORTION
  #transform all count of words to portion of text
  if(portion == TRUE){
    data[trim] = data[trim] / data$nword
  }
  return(data)
}

add_predictor = function(data = chart, new_pred, col_name, portion = TRUE){
  if(col_name %in% colnames(chart)){
    return(data)
  }
  data$new_col = rep(0, nrow(data))
  colnames(data)[ncol(data)] = col_name
  for(i in 1:nrow(data)){
    data[i, ncol(data)] = str_count(tolower(data$TEXT[i]), new_pred)
    if(portion == TRUE){
      data[i, ncol(data)] = data[i, ncol(data)] / data$nword[i]
    }
  }
  return(data)
}


test_predictor = chart
test_predictor = add_predictor(data = test_predictor, "omg", "omg")
test_predictor = add_predictor(data = test_predictor, ":\\)", "smile_face")
test_predictor = add_predictor(data = test_predictor, ";\\)", "wink_face")
test_predictor = add_predictor(data = test_predictor, "wtf", "wtf")
test_predictor = add_predictor(data = test_predictor, "fuck", "fuck")
test_predictor = add_predictor(data = test_predictor, "shit", "shit")
test_predictor = add_predictor(data = test_predictor, "\\$", "dollar_sign")
test_predictor = add_predictor(data = test_predictor, "1", "1")
test_predictor = add_predictor(data = test_predictor, "2", "2")
test_predictor = add_predictor(data = test_predictor, "3", "3")
test_predictor = add_predictor(data = test_predictor, "4", "4")
test_predictor = add_predictor(data = test_predictor, "5", "5")
test_predictor = add_predictor(data = test_predictor, "minus", "minus")
test_predictor = add_predictor(data = test_predictor, "\\+", "add_mark")
test_predictor = add_predictor(data = test_predictor, "\\-", "minus_mark")
write.csv(test_predictor, "train_1505_added.csv", row.names = FALSE)

for(i in ((which(colnames(test_predictor) == "nword"))+1):length(test_predictor)){
  if(sum(test_predictor[i] * data$nword[i])<50){
    print(colnames(test_predictor)[i])
  }
}

chart = test_predictor


sum(test_predictor[,"minus"])
sum(test_predictor[,"5"])
sum(test_predictor[,"4"])
sum(test_predictor[,"3"])
sum(test_predictor[,"2"])
sum(test_predictor[,"1"])
sum(test_predictor[,"$"])
sum(test_predictor[,":)"])
sum(test_predictor[,";)"])
sum(test_predictor[,"omg"])
sum(test_predictor[,"wtf"])
sum(test_predictor[,"fuck"])
sum(test_predictor[,"shit"])

head(test_predictor[(length(test_predictor)-40):length(test_predictor)])

chart = data_transform(chart, portion = TRUE)
trim = (which(colnames(chart) == "nchar")):length(chart)
head(chart)
```


```{r}
model1 = lm(STAR ~ . , data = chart[c(2,trim)])
#summary(model1)$coefficients[-1,4]
yhat = predict(model1)
yhat[yhat > 5] = 5
yhat[yhat < 1] = 1
resid = chart$STAR - yhat
SSE = sum(resid^2)
SSE
summary(model1)$p_value
plot(chart$Id, resid, xlab = "ID", ylab = "Residuals", main = "Residual Plot",pch = 23,bg = "red",cex = 1.2)
abline(a = 0, b = 0, col = "black", lwd = 3)
sortResid = sort(abs(resid), decreasing = TRUE)
Outlier = sortResid[1:100]
Outlier
```
  
```{r}
par(mfrow = c(1,2))
hist(resid, breaks = 100, main="Histogram of Residuals", xlab = "Residuals")
plot(density(resid), main="Kernel Density of Residuals", xlab = "Residuals")
```

```{r}
coefSum = rep(0, ncol(chart[trim]))
interSum = 0
count = 0

for (i in 1:200){
  print(i)
  list = sample(x = 1:nrow(chart), size = 5000)
  model.random = lm(STAR ~ ., data = chart[list, c(2, trim)])
  if(length(summary(model.random)$coefficients[-1,1]) == length(trim)){
    coefSum = coefSum + summary(model.random)$coefficients[-1,1]
    interSum = interSum + summary(model.random)$coefficients[1,1]
    count = count + 1
  }
  #else{
    #tail(summary(model.random))
  #}
}

write.csv(coefSum, "coefficient.csv", row.names = FALSE)
write.csv(interSum, "intercept.csv", row.names = FALSE)
fitRaw = data.matrix(chart[trim]) %*% data.matrix(coefSum/count)
fitRaw = fitRaw + interSum/count
fitRaw[fitRaw > 5] = 5
fitRaw[fitRaw < 1] = 1
residNew = chart$STAR - fitRaw
SSENew = sum(residNew^2)
SSENew

```


```{r}
test_data = read_csv("test_Madison.csv")
test_data = data_transform(test_data, portion = TRUE, test = TRUE, origin = chart)

test_data = read_csv("test_1519.csv")


trim_test = (which(colnames(test_data) == "nchar")):length(test_data)
```

```{r}
#coefSum = summary(model1)$coefficients[-1,1]
#interSum = summary(model1)$coefficients[1,1]
#count = 1
#length(coefSum/count)
#length(test_data[trim_test])
fit_data = data.matrix(test_data[trim_test]) %*% data.matrix(coefSum/count)
fit_data = fit_data + interSum/count
fit_data[fit_data > 5] = 5
fit_data[fit_data < 1] = 1
fit_data[fit_data > 4.5] = 5
result = as.data.frame(test_data[,1])
colnames(result)[1] = "Id"
result$Expected = fit_data

write.csv(result, "test_result.csv", row.names = FALSE)
```
